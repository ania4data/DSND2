{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodings\n",
    "\n",
    "Encodings are a set of rules mapping string characters to their binary representations. Python supports dozens of different encoding as seen here in [this link](https://docs.python.org/3/library/codecs.html#standard-encodings). Because the web was originally in English, the first encoding rules mapped binary code to the English alphabet. \n",
    "\n",
    "The English alphabet has only 26 letters. But other languages have many more characters including accents, tildes and umlauts. As time went on, more encodings were invented to deal with languages other than English. The utf-8 standard tries to provide a single encoding schema that can encompass all text.\n",
    "\n",
    "The problem is that it's difficult to know what encoding rules were used to make a file unless somebody tells you. The most common encoding by far is utf-8. Pandas will assume that files are utf-8 when you read them in or write them out.\n",
    "\n",
    "Run the code cell below to read in the population data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/population_data.csv', skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas should have been able to read in this data set without any issues. Next, run the code cell below to read in the 'mystery.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('mystery.csv')\n",
    "# Error\n",
    "#'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have gotten an error: **UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte**. This means pandas assumed the file had a utf-8 encoding but had trouble reading in the data file. \n",
    "\n",
    "Your job in the next cell is to figure out the encoding for the mystery.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf_16_be Worked\n",
      "                   gbk didn't Work\n",
      "                   bz2_codec didn't Work\n",
      "                   iso2022_jp_3 didn't Work\n",
      "                   ascii didn't Work\n",
      "                   cp1257 didn't Work\n",
      "                   iso8859_16 didn't Work\n",
      "                   mac_roman didn't Work\n",
      "                   cp1255 didn't Work\n",
      "                   johab didn't Work\n",
      "                   iso8859_3 didn't Work\n",
      "                   gb18030 didn't Work\n",
      "                   iso8859_2 didn't Work\n",
      "                   cp949 didn't Work\n",
      "                   iso2022_jp_ext didn't Work\n",
      "                   latin_1 didn't Work\n",
      "                   utf_8 didn't Work\n",
      "                   iso8859_4 didn't Work\n",
      "                   euc_jisx0213 didn't Work\n",
      "                   hz didn't Work\n",
      "utf_16_le Worked\n",
      "                   hex_codec didn't Work\n",
      "                   cp932 didn't Work\n",
      "                   tactis didn't Work\n",
      "                   utf_7 didn't Work\n",
      "                   uu_codec didn't Work\n",
      "                   cp1140 didn't Work\n",
      "                   cp1256 didn't Work\n",
      "                   iso2022_jp_2004 didn't Work\n",
      "                   cp424 didn't Work\n",
      "                   rot_13 didn't Work\n",
      "                   cp775 didn't Work\n",
      "                   cp950 didn't Work\n",
      "                   utf_32 didn't Work\n",
      "                   zlib_codec didn't Work\n",
      "                   iso8859_8 didn't Work\n",
      "                   cp865 didn't Work\n",
      "                   cp860 didn't Work\n",
      "                   iso8859_9 didn't Work\n",
      "                   cp869 didn't Work\n",
      "                   cp1026 didn't Work\n",
      "                   cp855 didn't Work\n",
      "                   euc_kr didn't Work\n",
      "                   shift_jis didn't Work\n",
      "                   cp437 didn't Work\n",
      "                   iso8859_15 didn't Work\n",
      "                   iso2022_jp didn't Work\n",
      "                   cp861 didn't Work\n",
      "                   cp858 didn't Work\n",
      "                   cp1250 didn't Work\n",
      "                   iso8859_6 didn't Work\n",
      "                   mac_turkish didn't Work\n",
      "                   cp1125 didn't Work\n",
      "                   koi8_r didn't Work\n",
      "                   mbcs didn't Work\n",
      "                   quopri_codec didn't Work\n",
      "                   euc_jis_2004 didn't Work\n",
      "                   cp273 didn't Work\n",
      "                   cp863 didn't Work\n",
      "                   gb2312 didn't Work\n",
      "                   cp500 didn't Work\n",
      "                   big5hkscs didn't Work\n",
      "                   cp857 didn't Work\n",
      "                   utf_32_be didn't Work\n",
      "                   cp1258 didn't Work\n",
      "                   ptcp154 didn't Work\n",
      "                   euc_jp didn't Work\n",
      "                   cp1252 didn't Work\n",
      "                   cp1251 didn't Work\n",
      "                   cp850 didn't Work\n",
      "                   cp862 didn't Work\n",
      "                   base64_codec didn't Work\n",
      "                   iso2022_jp_2 didn't Work\n",
      "                   iso8859_7 didn't Work\n",
      "                   utf_32_le didn't Work\n",
      "                   shift_jisx0213 didn't Work\n",
      "                   mac_greek didn't Work\n",
      "                   iso8859_13 didn't Work\n",
      "                   kz1048 didn't Work\n",
      "                   mac_cyrillic didn't Work\n",
      "utf_16 Worked\n",
      "                   iso2022_jp_1 didn't Work\n",
      "                   cp864 didn't Work\n",
      "                   hp_roman8 didn't Work\n",
      "                   iso8859_5 didn't Work\n",
      "                   big5 didn't Work\n",
      "                   tis_620 didn't Work\n",
      "                   mac_latin2 didn't Work\n",
      "                   cp037 didn't Work\n",
      "                   cp852 didn't Work\n",
      "                   iso8859_14 didn't Work\n",
      "                   iso8859_10 didn't Work\n",
      "                   cp866 didn't Work\n",
      "                   mac_iceland didn't Work\n",
      "                   cp1254 didn't Work\n",
      "                   iso8859_11 didn't Work\n",
      "                   cp1253 didn't Work\n",
      "                   iso2022_kr didn't Work\n",
      "                   shift_jis_2004 didn't Work\n"
     ]
    }
   ],
   "source": [
    "# TODO: Figure out what the encoding is of the myster.csv file\n",
    "# HINT: pd.read_csv('mystery.csv', encoding=?) where ? is the string for an encoding like 'ascii'\n",
    "# HINT: This link has a list of encodings that Python recognizes https://docs.python.org/3/library/codecs.html#standard-encodings\n",
    "\n",
    "# Python has a file containing a dictionary of encoding names and associated aliases\n",
    "# This line imports the dictionary and then creates a set of all available encodings\n",
    "# You can use this set of encodings to search for the correct encoding\n",
    "# If you'd like to see what this file looks like, execute the following Python code to see where the file is located\n",
    "#    from encodings import aliases\n",
    "#    aliases.__file__\n",
    "\n",
    "from encodings.aliases import aliases\n",
    "\n",
    "alias_values = list(set(aliases.values()))\n",
    "\n",
    "# TODO: iterate through the alias_values list trying out the different encodings to see which one or ones work\n",
    "# HINT: Use a try - except statement. Otherwise your code will produce an error when reading in the csv file\n",
    "#       with the wrong encoding.\n",
    "# HINT: In the try statement, print out the encoding name so that you know which one(s) worked.\n",
    "#from encodings import aliases\n",
    "# aliases.__file__\n",
    "# f=open('/opt/conda/lib/python3.6/encodings/aliases.py')\n",
    "# f.readlines()\n",
    "#alias_values {'ascii',\n",
    "#  'base64_codec',\n",
    "#  'big5',\n",
    "#  'big5hkscs',\n",
    "#  'bz2_codec',\n",
    "#  'cp037',\n",
    "\n",
    "for alias in alias_values:\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('mystery.csv',encoding=alias)\n",
    "        print('%s Worked' %alias)\n",
    "    except:\n",
    "        print(\"                   %s didn't Work\" %alias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>...</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>54211.0</td>\n",
       "      <td>55438.0</td>\n",
       "      <td>56225.0</td>\n",
       "      <td>56695.0</td>\n",
       "      <td>57032.0</td>\n",
       "      <td>...</td>\n",
       "      <td>101353.0</td>\n",
       "      <td>101453.0</td>\n",
       "      <td>101669.0</td>\n",
       "      <td>102053.0</td>\n",
       "      <td>102577.0</td>\n",
       "      <td>103187.0</td>\n",
       "      <td>103795.0</td>\n",
       "      <td>104341.0</td>\n",
       "      <td>104822.0</td>\n",
       "      <td>105264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>8996351.0</td>\n",
       "      <td>9166764.0</td>\n",
       "      <td>9345868.0</td>\n",
       "      <td>9533954.0</td>\n",
       "      <td>9731361.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27294031.0</td>\n",
       "      <td>28004331.0</td>\n",
       "      <td>28803167.0</td>\n",
       "      <td>29708599.0</td>\n",
       "      <td>30696958.0</td>\n",
       "      <td>31731688.0</td>\n",
       "      <td>32758020.0</td>\n",
       "      <td>33736494.0</td>\n",
       "      <td>34656032.0</td>\n",
       "      <td>35530081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>5643182.0</td>\n",
       "      <td>5753024.0</td>\n",
       "      <td>5866061.0</td>\n",
       "      <td>5980417.0</td>\n",
       "      <td>6093321.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21759420.0</td>\n",
       "      <td>22549547.0</td>\n",
       "      <td>23369131.0</td>\n",
       "      <td>24218565.0</td>\n",
       "      <td>25096150.0</td>\n",
       "      <td>25998340.0</td>\n",
       "      <td>26920466.0</td>\n",
       "      <td>27859305.0</td>\n",
       "      <td>28813463.0</td>\n",
       "      <td>29784193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>1608800.0</td>\n",
       "      <td>1659800.0</td>\n",
       "      <td>1711319.0</td>\n",
       "      <td>1762621.0</td>\n",
       "      <td>1814135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2947314.0</td>\n",
       "      <td>2927519.0</td>\n",
       "      <td>2913021.0</td>\n",
       "      <td>2905195.0</td>\n",
       "      <td>2900401.0</td>\n",
       "      <td>2895092.0</td>\n",
       "      <td>2889104.0</td>\n",
       "      <td>2880703.0</td>\n",
       "      <td>2876101.0</td>\n",
       "      <td>2873457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AND</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>13411.0</td>\n",
       "      <td>14375.0</td>\n",
       "      <td>15370.0</td>\n",
       "      <td>16412.0</td>\n",
       "      <td>17469.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83861.0</td>\n",
       "      <td>84462.0</td>\n",
       "      <td>84449.0</td>\n",
       "      <td>83751.0</td>\n",
       "      <td>82431.0</td>\n",
       "      <td>80788.0</td>\n",
       "      <td>79223.0</td>\n",
       "      <td>78014.0</td>\n",
       "      <td>77281.0</td>\n",
       "      <td>76965.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Country Name Country Code     Indicator Name Indicator Code  \\\n",
       "0           0        Aruba          ABW  Population, total    SP.POP.TOTL   \n",
       "1           1  Afghanistan          AFG  Population, total    SP.POP.TOTL   \n",
       "2           2       Angola          AGO  Population, total    SP.POP.TOTL   \n",
       "3           3      Albania          ALB  Population, total    SP.POP.TOTL   \n",
       "4           4      Andorra          AND  Population, total    SP.POP.TOTL   \n",
       "\n",
       "        1960       1961       1962       1963       1964     ...      \\\n",
       "0    54211.0    55438.0    56225.0    56695.0    57032.0     ...       \n",
       "1  8996351.0  9166764.0  9345868.0  9533954.0  9731361.0     ...       \n",
       "2  5643182.0  5753024.0  5866061.0  5980417.0  6093321.0     ...       \n",
       "3  1608800.0  1659800.0  1711319.0  1762621.0  1814135.0     ...       \n",
       "4    13411.0    14375.0    15370.0    16412.0    17469.0     ...       \n",
       "\n",
       "         2008        2009        2010        2011        2012        2013  \\\n",
       "0    101353.0    101453.0    101669.0    102053.0    102577.0    103187.0   \n",
       "1  27294031.0  28004331.0  28803167.0  29708599.0  30696958.0  31731688.0   \n",
       "2  21759420.0  22549547.0  23369131.0  24218565.0  25096150.0  25998340.0   \n",
       "3   2947314.0   2927519.0   2913021.0   2905195.0   2900401.0   2895092.0   \n",
       "4     83861.0     84462.0     84449.0     83751.0     82431.0     80788.0   \n",
       "\n",
       "         2014        2015        2016        2017  \n",
       "0    103795.0    104341.0    104822.0    105264.0  \n",
       "1  32758020.0  33736494.0  34656032.0  35530081.0  \n",
       "2  26920466.0  27859305.0  28813463.0  29784193.0  \n",
       "3   2889104.0   2880703.0   2876101.0   2873457.0  \n",
       "4     79223.0     78014.0     77281.0     76965.0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "There are dozens of encodings that Python can handle; however, Pandas assumes a utf-8 encoding. This makes sense since utf-8 is very common. However, you will sometimes come across files with other encodings. If you don't know the encoding, you have to search for it.\n",
    "\n",
    "Note, as always, there is a solution file for this exercise. Go to File->Open.\n",
    "\n",
    "There is a Python library that can be of some help when you don't know an encoding: chardet. Run the code cells below to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install the chardet library\n",
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'UTF-16', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# import the chardet library\n",
    "import chardet \n",
    "\n",
    "# use the detect method to find the encoding\n",
    "# 'rb' means read in the file as binary\n",
    "with open(\"mystery.csv\", 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
